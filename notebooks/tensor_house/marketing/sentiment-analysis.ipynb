{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eligible-vertical",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using Transformers\n",
    "\n",
    "This notebook demonstrates how to perform classification of user-generated content (e.g. product reviews) using transformers. \n",
    "\n",
    "### Use Case\n",
    "We have a lage amount of user-generated content such as product reviews, call transcripts, or social media posts. We want to create a model that assigns labels to individual content items. For example, movie reviews can be assigned with `positive` and `negative` sentiment labels. We assume that we have a significant amount of labeled training data.\n",
    "\n",
    "### Prototype: Approach and Data\n",
    "We implement the sentiment classification model using a transformer that is trained from scratch on a labeled dataset. The implementation is based on [1]. \n",
    "\n",
    "We use the IMDB dataset that contain 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). The dataset is automatically downloaded using Keras repository.  \n",
    "\n",
    "### Usage and Productization\n",
    "This prototype can be used to evaluate how well a model trained from scratch can perform a given text classification task. The default dataset can be easily replaced with custom labeled dataset that contains enough training samples.\n",
    "\n",
    "In practice, one would typically use a pretrained model or LLM service to perform sentiment analysis and other text classification tasks.\n",
    "\n",
    "### References\n",
    "1. https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quick-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Imports and settings\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'pdf.fonttype': 'truetype'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "streaming-first",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n",
      "Review [to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an] -> Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 05:53:54.519789: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 67s 84ms/step - loss: 0.3903 - accuracy: 0.8125 - val_loss: 0.3378 - val_accuracy: 0.8579\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 65s 83ms/step - loss: 0.1955 - accuracy: 0.9259 - val_loss: 0.3140 - val_accuracy: 0.8731\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load the dataset\n",
    "#\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200        # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "#\n",
    "# Preview the dataset\n",
    "#\n",
    "movie_index = 0 \n",
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[movie_index])\n",
    "sentiment = 'Positive' if y_train[movie_index] == 1 else 'Negative'\n",
    "print(f'Review [{decoded_sequence}] -> {sentiment}')\n",
    "\n",
    "#\n",
    "# Model components\n",
    "#\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "#\n",
    "# Model specification\n",
    "#\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2   # Number of attention heads\n",
    "ff_dim = 32     # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "#\n",
    "# Model training\n",
    "#\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "noted-johnson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review [to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an] -> [[0.00153198 0.998468  ]] (Positive)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Test scoring\n",
    "#\n",
    "x = x_train[movie_index]\n",
    "y = y_train[movie_index]\n",
    "class_p = model.predict(np.atleast_2d(x))\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x)\n",
    "true_sentiment = 'Positive' if y == 1 else 'Negative'\n",
    "print(f'Review [{decoded_sequence}] -> {class_p} ({true_sentiment})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
